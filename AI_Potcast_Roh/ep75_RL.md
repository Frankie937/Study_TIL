(EP 75. (수학 공식 없는) 강화 학습 이야기 : https://www.youtube.com/watch?v=7pGfU6AC4XY )

# 강화학습 이야기: Kimi K2와 포스트 트레이닝 시대의 LLM 추론

2025년 11월 8일, 중국 Moonshot AI의 Kimi K2 Thinking 모델이 발표되면서 AI 업계에 큰 파장이 일었다. GPT-5와 Claude Sonnet 4.5를 능가하는 벤치마크 성적을 기록하며, 포스트 트레이닝 시대의 빠른 모델 발전을 보여주고 있다. 이번 영상은 강화학습(Reinforcement Learning, RL)의 핵심 개념부터 시작해, LLM이 어떻게 추론 능력을 획득하는지 그 메커니즘을 수학 공식 없이 쉽게 풀어낸다.

### 1. Kimi K2와 포스트 트레이닝 시대

음, Kimi K2 Thinking 모델은 1조(trillion) 개의 파라미터를 가진 MoE(Mixture-of-Experts) 아키텍처를 사용하지만, 실제로는 각 추론마다 320억 개의 파라미터만 활성화된다. 이전 Kimi 모델이 9월에 나왔는데, 불과 두 달 만에 추론(thinking) 모델까지 발표된 셈이다. 

함, 이제는 프리트레이닝(pre-training)보다 포스트 트레이닝(post-training)의 레시피가 경쟁력의 핵심이 되었다. 모델 개발 주기가 빨라지면서 각 기업은 어떤 포스트 트레이닝 기법을 사용하는지를 공개하지 않게 되었고, 이것이 바로 그들의 노하우가 되었다.

### 2. MoE와 양자화를 통한 효율성

그래서 Kimi K2는 INT4 양자화를 사용해 MoE 부분을 압축했다. MoE 모델은 각 모듈이 전체 데이터가 아닌 일부만 학습하기 때문에, 일반 dense 모델보다 양자화가 잘 된다는 특징이 있다. 실제로 1조 파라미터 모델이지만 양자화 후에는 약 500GB 정도의 크기로 줄어들어, 8개 GPU를 가진 단일 머신에서도 실행 가능하다.

### 3. 강화학습의 기본 개념

강화학습은 에이전트가 환경에서 행동(action)을 취하고, 그 결과로 보상(reward)을 받으며 학습하는 방식이다. 지도학습과 다른 점은, 전문가가 어떻게 행동해야 하는지 가르쳐주지 않는다는 것이다. 자율주행 비유를 들면, 지도학습은 사람이 운전한 궤적을 그대로 따라하도록 학습시키는 것이고, 강화학습은 목적지에 도착하는 것만 보상으로 주고 어떻게 운전할지는 스스로 찾게 하는 것이다.

### 4. Credit Assignment Problem

함, 강화학습에서 가장 어려운 문제 중 하나가 신용 할당 문제(Credit Assignment Problem)다. 여러 행동을 취한 후 보상을 받았을 때, 그 중 어떤 행동이 실제로 도움이 되었는지 파악하는 게 쉽지 않다. 사람도 이 부분에서 많은 실수를 한다. 도박에서 패턴이 없는데 패턴을 찾으려 하거나, 관련 없는 행동이 도움이 되었다고 착각하는 것처럼 말이다.

### 5. LLM에 강화학습이 도입된 이유: RLHF

음, LLM에 강화학습이 본격적으로 도입된 것은 RLHF(Reinforcement Learning from Human Feedback) 형태였다. 기본 아이디어는 이렇다. LLM에게 프롬프트를 주고 여러 응답을 생성하게 한 다음, 사람이 어떤 응답이 더 좋은지 라벨링한다. 이 선호도 데이터로 보상 모델(reward model)을 학습시키고, 이 보상 모델을 사용해 LLM을 강화학습으로 fine-tuning한다.

### 6. SFT의 한계와 할루시네이션 문제

그래서 왜 RLHF를 해야 할까? SFT(Supervised Fine-Tuning)만으로는 안 될까? SFT는 전문가가 정답을 작성하면 모델이 그것을 따라하도록 학습하는 방식이다. 문제는 LLM과 전문가가 서로 다른 에이전트라는 점이다.

함, 모델이 이미 아는 질문으로 학습하면, "아는 내용은 바로 답하면 된다"는 패턴을 학습한다. 하지만 모델이 모르는 질문으로 학습하면, 정답 자체도 배우지만 동시에 "모를 때도 일단 답하면 된다"는 나쁜 패턴도 함께 학습하게 된다. 이것이 할루시네이션(hallucination)의 주요 원인 중 하나다.

### 7. On-Policy vs Off-Policy

음, 이것은 On-Policy와 Off-Policy의 차이와도 관련이 있다. On-Policy는 학습하는 에이전트와 행동하는 에이전트가 같은 것이고, Off-Policy는 다른 것이다. SFT는 전문가(행동 에이전트)의 경험으로 모델(학습 에이전트)을 학습시키는 Off-Policy 방식이다. 

그래서 전문가는 A에서 B로 갈 수 있지만, 모델은 그 능력이 없어서 C로 가게 되면 학습 때 본 적 없는 상황이 발생한다. 이런 Off-Policy의 문제가 모델에게 해결할 수 없는 문제를 해결할 수 있다고 가정하고 학습시킬 때 나타난다.

### 8. 일반화를 위한 조건

함, 일반화(generalization)가 가능하려면 모델에게 해결할 수 있는 문제를 주어야 한다. 간단한 예로, 각 레이어가 덧셈 하나를 수행할 수 있는 신경망이 있다면, 2개 레이어는 두 번의 덧셈까지만 해결 가능하다. 세 번의 덧셈이 필요한 문제는 해결할 수 없어서 그냥 암기하게 된다. 하지만 3개 레이어로 늘리면 오히려 오버피팅이 줄어든다.

### 9. 인터넷 데이터의 한계

그래서 문제는 인터넷 데이터에 중간 과정이 거의 없다는 것이다. Math Stack Exchange의 유명한 사례를 보면, Cleo라는 유저가 복잡한 적분 문제를 중간 과정 없이 바로 답만 제시했다. 인터넷 데이터는 대부분 이런 식이다. 질문과 답만 있고, 그 사이의 추론 과정(trajectory)은 없다.

### 10. 프리트레이닝과 탐색 공간의 축소

음, 그럼에도 프리트레이닝이 중요한 역할을 한다. Kimi K2의 경우 학습 loss가 약 1.32인데, 이를 perplexity로 환산하면 약 3.7이다. 원래 어휘 크기가 163,840개라면 모든 토큰에 동일한 확률을 부여할 때 선택지가 163,840개인데, 프리트레이닝을 통해 약 3.7개 중에서 하나를 고르는 문제로 축소된다.

### 11. 추론의 분기점: Entropy가 높은 토큰

함, 실제 추론 과정을 보면 대부분의 토큰은 파란색(낮은 엔트로피)이고, 소수만 빨간색(높은 엔트로피)이다. 놀랍게도 숫자 계산 같은 건 엔트로피가 낮다. 오히려 "maybe", "however" 같이 사고의 흐름을 바꾸는 토큰이 엔트로피가 높다. 이런 고엔트로피 토큰이 추론의 분기점(fork)이 되어 다양한 추론 경로를 만든다.

### 12. 집단적 추론의 학습

그래서 모델이 어떻게 이런 추론 능력을 얻었을까? 인터넷 포럼 데이터가 중요한 역할을 한다. 예를 들어 학생이 숙제를 가져오면, 전문가가 답을 바로 주지 않고 "이 경우는 어떻게 생각해?"라며 계속 생각하게 만든다. 학생이 "여기서 실수한 것 같아요"라고 하면 또 다른 힌트를 준다. 이것이 전형적인 추론 모델의 패턴이다.

### 13. 강화학습을 통한 추론 능력의 발현

음, 결국 프리트레이닝을 통해 모델은 이미 추론 패턴을 가지고 있다. 하지만 확률이 매우 낮다. "뉴턴은 홀수년에 태어났나요?"라는 질문에 바로 "짝수"라고 답할 확률이 가장 높지만, "뉴턴은 1643년에 태어났고, 1643은 홀수이므로 홀수년"이라고 생각하며 답하는 패턴도 존재한다. 다만 확률이 낮을 뿐이다.

강화학습은 이 낮은 확률의 추론 패턴을 끌어올린다. DeepSeek R1의 경우 순수 RL만으로도 놀라운 추론 능력이 나타났다. 다만 가독성 문제와 언어 혼합 문제가 있어서, 최종 모델은 소량의 cold-start 데이터와 함께 multi-stage 학습을 진행했다.

### 14. 성공적인 강화학습의 조건

함, 성공적인 강화학습을 위해서는 정확한 피드백이 중요하다. OpenAI o1의 경우 process reward model을 사용해 각 추론 단계를 평가한다. 또한 고엔트로피 토큰에 집중해서 학습하면 효율성이 크게 향상된다는 연구 결과도 있다. 전체 토큰의 20%인 고엔트로피 토큰만 업데이트해도 전체를 업데이트한 것과 비슷하거나 더 나은 성능을 보인다.

### 15. 추론 토큰과 test-time scaling

그래서 o1이나 DeepSeek R1 같은 추론 모델은 "thinking tokens"를 생성한다. 사용자에게는 보이지 않지만 모델이 내부적으로 추론 과정을 거친다. OpenAI는 약 25,000 토큰을 추론용으로 사용할 것을 권장한다. 

음, 추론 토큰 길이를 늘리면 성능이 향상되는 test-time scaling 현상도 관찰된다. 하지만 무한정 늘린다고 좋은 건 아니고, 최적 길이가 존재한다. 너무 길어지면 오히려 "overthinking" 현상이 나타나 성능이 떨어질 수 있다.

### 마무리

결국 이 모든 것이 시사하는 바는 명확하다. 강화학습은 LLM이 이미 프리트레이닝에서 얻은 추론 능력을 끌어내는(elicit) 역할을 한다. On-policy RL로 모델 자신의 경험을 통해 학습하면 일반화 가능한 패턴을 배울 수 있고, 정확한 피드백과 함께 고엔트로피 토큰에 집중하면 효율적으로 추론 능력을 향상시킬 수 있다. 

포스트 트레이닝 시대에는 이런 RL 레시피가 각 기업의 핵심 경쟁력이 될 것이고, Kimi K2가 보여준 것처럼 오픈소스 모델도 충분히 최고 수준의 성능에 도달할 수 있다. 인생도 on-policy RL처럼, 자신의 경험을 통해 배우고 성장하는 과정이 아닐까 하는 생각이 든다.
