(영상 :https://www.youtube.com/watch?v=iDd-7trU3VE&t=31s)

# DeepSeek이 바꿔버린 2025년: MoE와 RLVR, 그리고 에이전트의 진화
2025년 초 DeepSeek-R1의 등장 이후, AI 생태계는 급격한 패러다임의 변화를 겪었다. 특히 중국발 오픈 프런티어 모델들이 시장을 주도했으며, 기술적으로는 MoE(Mixture of Experts) 아키텍처의 표준화와 RLVR(Reinforcement Learning with Verifiable Rewards)을 통한 에이전트 포스트 트레이닝이 핵심 흐름으로 자리 잡았다. 이 영상은 제한된 연산 자원 속에서 어떻게 프런티어급 성능을 달성했는지에 대한 '레시피'와 강화학습(RL)의 본질적인 역할을 다룬다.

### 1. DeepSeek 모멘트와 오픈 프런티어 모델의 부상
2025년의 가장 큰 변화는 DeepSeek, MiniMax, Alibaba, Tencent 등 수많은 중국 기업들이 '프런티어급' 모델을 잇달아 공개했다는 점이다. 2024년까지는 자원의 한계로 인해 Llama 2 수준(70B)의 적당한 모델을 만드는 것이 목표였다면, 2025년에는 DeepSeek이 제한된 연산 자원(800~2,000대 GPU)으로도 프런티어 성능을 낼 수 있음을 증명하며 모두가 더 강력한 모델을 지향하게 되었다. 반면 미국과 중국을 제외한 다른 국가에서는 이렇다 할 오픈 프런티어 모델이 나오지 못하며, 중국이 오픈 모델 생태계를 주도하는 형국이 되었다.
​

### 2. MoE(Mixture of Experts)의 압도적 효율성
2025년 출시된 대부분의 모델은 MoE 아키텍처를 채택했다. 영상에서 제시된 그래프에 따르면, 동일한 10^24 FLOPs(연산량)를 투입했을 때 MoE 모델은 일반적인 Dense 모델 대비 약 7배 이상의 성능 효율(Compute Multiplier)을 보여준다.
​
* 희소성(Sparsity)의 위력: 전체 파라미터 중 일부(예: 1/50)만 활성화하는 희소성이 높을수록 연산 효율 배수는 더욱 커지는 경향(Scaling Law)을 보인다.
​* 표준이 된 아키텍처: DeepSeek이 정립한 MoE 아키텍처가 너무나 효율적이었기에, Moonshot의 Kimi 등 다른 기업들도 독자적인 구조를 개발하기보다 DeepSeek의 아키텍처를 그대로 채택하여 베이스로 삼는 현상이 나타났다.​


### 3. RLVR과 에이전트 포스트 트레이닝
DeepSeek-R1과 함께 등장한 **RLVR(Reinforcement Learning with Verifiable Rewards)**은 2025년의 또 다른 핵심 키워드다. 이는 모델이 도구(Tool)를 사용하고 환경과 상호작용하는 과정을 '결과 중심'으로 학습시키는 방법론이다.
​
* 결과 기반 평가: 코딩 에이전트를 예로 들면, 모델이 어떤 과정을 거쳤는지는 중요하지 않다. 최종 결과물이 유닛 테스트(Unit Test)를 통과했는지 여부만으로 보상(Reward)을 부여한다.
* 에이전트 학습: 이를 통해 모델은 사람이 일일이 가르치지 않아도, 스스로 도구를 사용하고 결과를 수정하며 목표를 달성하는 '에이전트'로서의 능력을 포스트 트레이닝 단계에서 습득하게 되었다.
​
### 4. 강화학습의 본질: Atomic Skill과 조합 능력
2025년은 강화학습(RL)이 LLM에 부여하는 능력이 무엇인지에 대한 이해가 깊어진 해였다.

* Atomic Skill (원자적 기술): 사칙연산이나 기본 코딩 문법과 같은 개별적인 단위 기술은 프리트레이닝(Pre-training) 단계에서 학습한다.​
* Combinatorial Skill (조합 능력): RL은 이러한 원자적 기술들을 적재적소에 배치하고 순서대로 엮어서(Chaining) 복잡한 문제를 해결하는 '조합 능력'을 길러준다.
​
즉, RL이 없는 능력을 새로 만들어내는 것이 아니라, 프리트레이닝된 잠재력을 최대한 끄집어내어 문제 해결에 활용하도록 만드는 과정임이 밝혀졌다.
​
### 5. 2026년 전망: 스케일업과 지속 학습
2026년에는 모델의 덩치를 키우는 '스케일업'과 더불어 데이터와 학습 방법론의 고도화가 예상된다.

* 데이터 병목과 롱테일: 99%의 성능을 99.9%로 올리기 위한 고품질 데이터 확보가 중요해지며, 이는 단순한 크기 경쟁을 넘어선 '암묵지'의 영역이 될 것이다.
* 지속 학습(Continual Learning): 모델이 스스로 무엇을 배워야 할지 탐색하고, 자신과 대결하는 셀프 플레이(Self-play)를 통해 데이터를 생성하며 발전하는, 더 자율적인 에이전트로 진화할 것으로 전망된다.
