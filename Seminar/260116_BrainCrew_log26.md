(아직 정리중 ... 수정해야 함) 

---
이벤트링크: https://luma.com/ipd0cijk
발표자료 : https://github.com/teddynote-lab/Log26_n_Connect2026.git



---

![image.png](attachment:47de6047-1bd7-4dcb-b051-ecf20e106e10:image.png)

### Opening (Teddy)

- welcome to log26
    - 그동안 해왔던 프로젝트를 로깅한다는 의미
- Braincrew - 2018 창립
    - 교육팀 & teddynote lab(product -솔루션/ data pasing/ rag)
- Product
    - 현장경험, 검증 빠진 솔루션은 실패확률 높음
    - 외주를 많이 해서 되게끔 (고객사의 어려움을 해결하는 - 현장 경험)
        - 열심히 개발 > 노하우 습득 > 제품에 반영 > 제품 개선
    - 종류:  weagent (agent platform), langconnect, documnet parser, forward deployed engineer, agent builder (n8n, dify 등 오픈소스에 대한 보안 등의 문제 불안감이 있음)
    - agent builder의 좋은 조건?
        - 누구나 쉽게 사용
        - 우수한 성능, 에이전트 공유, MCP/A2A지원, 확장성, 멀티에이전트 협업, 확장성, 운영 및 인프라, 온프레미스 지원 등등
    - deep agent builder 소개(1월말 출시 - 2월 가오픈 - 3월 엔터프라이즈 향으로 오픈) : 예시 - 웹검색에이전트 - 사용자 의도 수집, 툴 선택, 미들웨어추천
        - fix기능 - 자연어로 설정옵션 변경 가능하도록

### Data Parsing (Peter) - document structuring

- braincrew - 데이터팀 리더 AI research Engineer
- AI comprehension
1. Structuring의 중요성
    - RAG가 다 해준다? No
        - 가장 중요한건 chunking
        - 다양한 청킹 방법론들이 많음 (중요한 건 문서 구조를 잘 이해하고 그 구조에 맞게 잘 청킹해야 함)
            - 그래서 structuring이 중요함!!!!!
            - 문서는 원래 구조를 가지고 있음
            - 검색 시 작은 청크, 답변 생성 시에는 큰 청크가 유리함 (굉장히 공감!!! )
            - 하위계층을 검색하더라도 상위계층을 함께 전달가능
            - 정보이론 관점에서도 질문은 보통 고밀도, 문서같은 경우에는 저밀도 특성을 갖고 있음
                - 문서 하위계층 -고밀도, 문서 상위계층 - 저밀도
            - 하나의 청크에 여러 다양한 주제가 섞여있으면 평균화 오류
                - 각 벡터는 한가지 주제에만 집중된 분포를 가짐
    - 검색 점수의 long tail 방지
    - 실제 프로젝트에서 structuring이 recall 85%
    - MoC 논문 참고
        - 청크 성능 평가 방법 제안
        - BC - 청크가 청크끼리 구분이 가능한지 (문장과 문장 사이의 semantic distance 를 보고 청크를 끊음)
        - CS- 하나의 청크 안에 문장들일 잘 다루고 있는지
        - 
2. Regex로 구조잡기 
- Heading의 규칙을 찾아 청킹
- regex 한계
    - 문서하나하나 특징을 사람이 직접 파악
    - 규칙이 아무리 촘촘해도 문서마다 다름
    - 문서에 목차가 있으면 잘될 수 있음 !!!
    
1. LLM으로 구조 추론 하기 
- chunking은 전역 최적화 문제
- llm은 로컬 확률 모델
    - transformers는 next token prediciton
    
     
    
1. 구조를 학습시키는 모델링 접근 
    1. 문서를 넣으면 알아서 구조화하도록 학습 시키는 
    2. 아키텍처 : texts > instruction tuning > CE loss > culmulative 
    3. Qwen3.7 ~ 모델
    4. 다양한 형태의 문서를 구조화할 경우 어떻게 dataset을 준비하셨는지? 

1. lazy chunking  
    1. 모든 문서에 대해서 청킹을 해야 할까?
        1. 어떤 질문을 더 많이 하지? 라는 고민을 하는 것 
            1. 다큐먼트 레벨 질문
            2. 청크레벨 질문 
                1. 정보 손실 없는 벡터 공간의 슬림화 
                2. 아키텍처 need chunking 라우팅 노드 추가 : 기준을 토큰 수로 잡음 (recursive 청킹 사용) 
    2. 장점 : 메모리 효율적/ 검색 공간의 최적화 가능/ 원본 맥락 유지 가능 
    3. 단점: 지연시간, 부하가 걸릴 수 있음 
2. 정리 
    1. 구조정보 -ocr 사용
3. 브레인크루 데이터 팀 방향성 
    1. AI comprehension : 문서의 구조정보와 의미정보를 동시에 다루는 모델 필요 
    2. 일반화의 시작점이 될 것 
4. 결론
    1. 청킹기법 굉장히 중요
    2. 문장구조와 의미구조를 모두 사용해야 함
    3. 청킹의 일반화를 지향 
    4. structuring 에 대한 소개였음 
5. QnA 
    1. 베이스라인을 잡은 거지 CE loss 다중분류로 정했기에) 를 사용한 것, 다른 loss 기법도 적용 및 테스트 하고 있음 
    2. 전역과 로컬에 대한 관점 
        1. 정답은 아님, 전역이라고 했다는 이유가 문서 전체를 바라봤기 때문에 전역이라고 생각 
    3. cosine similarity를 쓰지 않은 이유? 차이점을 실제로 보여주기 위해  
    4. 건설업체 dataset - 고객사에게 평가셋을 요청 
    5. 문서 구조화 학습할 때 어떻게 dataset을 구축했는지? 
        1. 답변이 아닌 것 같음 - vlm을 쓰는게 쉬웠음 slide ~ 구글 논문 

### Intentrix (Stella)

- llm 서비스 개발 과정에서 마주하는 모호함을 다루는 방법

- INTENTrix 소개
    - 광고마케팅 분야의 의도 분류
    - 마케팅 부서에서 여러 인텐트 항목이 많음
    - FGI (Focus Group Interview)  - 소비자의 심층적인 인사이트, 동기 등등 분류(모더레이터가 설계된 질문을 바탕으로 사용자에게 인텐트를 끌어내는 방식) 참고
    - 소비자의 intent수집이 목적
        - 다양한 소비자들의 발화 데이터 = 알고 싶은 진짜 소비자들의 인텐트
        - ai챗봇 기반 intent 수집 환경 구축
- 모호함을 마주했을 때
    - AX 기획 및 설계
        - zero to one 기획 설계의 막막함
            - 초기기획 및 설계 > poc > 도메인 전문가 직접 테스트 > 피드백 > poc개선 (빠르게 피드백 받고 poc라도 계속 개선하는 방식) — 이러면서 설계 방향성을 구체화함
            - 정성 피드백 (정량 평가를 위한 golden set을 정의하기 어려움)
            - 점진적 개선 사례
                - 질문의 순서나 흐름이 자연스러워야 함
                - 모든 문항을 질문하여 응답을 얻어야 하고 그게 모두 db에 저장되어야 함 but 응답 및 상황에 다라 유동적인
                    - 질문 - 응답 상태 판단 : waiting/passed/answered + auto_filled
                        - passed > 응답자가 다음 질문의 답변을 미리 한경우 에도 매칭을해줘야 해서 auto_filled 상태도 추가
            
        - 도메인 지식 기반으로 명확한 업무 프로세스 이해 필요
        - 자연어의 모호함
            - intent 정의는 llm을 통해 구분하기 다소 추상적 > 문제해결되지 않음
            - 솔루션: 객관적 판단이 가능한 수준의 명시적인 요소로 구조화 / 발화문에서 intent를 판단하기 위한 정보를 sturcutred ouput 형태로 추출
            - llm as a judge 로 평가 결과 + 실무자의 정성 평가 결과
            - 한국어의 특성 : 다의어/고맥락 언어
                - 정확한 표현/단어 사용하도록 하는게 어려우니,  충분한 맥락을 반영하도록 !!!
            - context engineering
- 결론
    - 넥스트 스텝(고도화 예정)
        - 채팅 시나리오 기반의 정량 평가 하고자 함
        - 챗봇 참여자의 메세지에 따라 다양하고 유연한 대처 필요 : workflow 구조에서 agent구조로 변경하고 비교실험

### RAG 성능 고도화 : 제조, 금융, Lifelog 분야 (Jaehun)

- 최재훈님
- RAG하기 어려운 도메인….
- 실패사례 공유- 유의미한 정보라고 생각

1. 제조 
    1. 소스 도큐먼트가 별다른 특정 도메인이 없는 건지 
    2. sub domain 
        1. 각 소스 도큐먼트의 유사도를 측정해 봄 
        2. edge case 
            1. linguistic difference
            2. document layout
            3. ambiguous category by domain 
    3. 멀티 모달 처리 어떻게? 
        1. 연쇄적으로 
        2. 메타데이터 설계를 어떻게 해야 하는가? 
            1. 데이터를 위한 데이터인 메타데이터를 최대한 세부적으로 
            2. 청크 전략 /청쿠 구성(structure)
            3. 법정문서나 건설문서 등 여러 조항들이 있는 구조를 파악해서 청크도 구성을 다시 
    4. 실패 사례 ?
        1. get vocab (도메인에 적합한 단어를 찾아오는 것) : 크게 성능차이가 없었음 
            1. 임베딩 모델에 대한 domain adoption이 아니면 크게 유의미하지 않을 것 같음 
        2. get decsion 
            1. 도메인 시나리오 - 정확도는 높지만 강건성을 떨어짐 …
        3. llm-based reranking - 
2. 금융
    1. 대출심사를 받을 때 리포트 포맷이 있는데 그걸 잘 작성하도록 하는 프로젝트
    2. 테이블 데이터 llm이 어떻게 잘 이해할 수 있도록 하는지 관건이었음!!! 
        1. 마크다운 key-value 포맷 적용 
        2. toon 포맷, 
3. lifelog
    1. 개인의 행동 패턴을 가지고 qa 시스템을 만든 프로젝트 (챌린지가 넘쳐났음) 
    2. behacior-pattern data? 
        1. 어떠한 사람이 특정 날짜, 시간대에 특정 장소, 행동 등 일련의 시간마다 정보들을 캡처해놓은 정보 
            1. 내가 1주일 동안 커피를 몇 번 마셨어? 
    3. general/statistcs query 
    4. graph-traverse(지연시간 오래걸림), dense retriever (재현성..)
    5. metadata extraction 
    
4. FAQ
    1. RAG할 때 중요한 건? 평가 (평가가 제대로 정립되지 않으면 판단을 내래기 어려움)
    2. 답변에 대한 평가는 llm as a judge로 하긴 하는데 다양한 모델을 상용하고  이후에 
    3. 요즘은 MCP/Agnet가 대세 아닌가요?’
        1. 적용하고자 하는 환경과 목표 그리고 특성을 고려하여 적용하길 권함 
5. QnA 
    1. 시나리오 베이스 리트리버 3가지 충족 했다고 하는데 더 자세히 설명해줄 수 있는지? 
        1. general query / statistics query

---

## Advanced Systems

![image.png](attachment:8cb5834b-36fb-455d-a301-007fe59a3cf0:image.png)

### Long-term Memory MCP (Sung)

김성연님

사내 솔루션 기반의 개인화된 장기 기억 메모리 MCP 개발 사례 

1. 도입 - 왜 장기메모리가 중요한가?
    1. AI 기억상실 문제 
        1. 사용자 프로필 기억
        2. 문서 시맨틱 검색 
        3. 선호도 기반 응답 
2. 시스템 설계
    1. 왜 mcp? - 플랫폼 종속성 벗어나기 위함 
    2. mcp 4가지
        1. 마이크로서비스 친화적
        2. 표준 프로토콜의 힘
        3. 네이티브 멀티테넌트 
        4. 실시간 양방향 통신 
    3. graphDB를 처음에 시도했으나, 비용 많이 들고, 운영 복잡도가 높고 mcp내 llm 호출 필요하기에 또 비용 발생 
    4. 그래서, postgreSQL 사용
        1. 간결한 json 포맷 설계 
        2. pgvector
        3. 단점? 구조를 단순하게 가서 아직 발견하지 못했으나, 구조가 복잡하게 가져가게 되면 검색하게 되면 속도가 좀 느려지는 이슈가 발생할 수 있을 것 같음 (데이터 규모는 poc단계에서 크게 가져가지 않았음 - 사내 인원이 사용할 수 있을 정도로?) 
3. 핵심 기능 및 적용사례
    1. add_memory 
    2. load_memory 
    3. add_document 
    4. search_documnets
4. 개발회고
- 경험과 기술
- 교훈
    - mvp로 빠르게 만들고 피드백 받고 확장
    - 비용과 인프라 함께 고려 (비용, 인프라, 운영 복잡도 등 고려 필요)
    - 고객별 요구사항 우선순위 정확히 파악
    - 인프라 학습을 병행 (*좋은 코드도 배포가 안되면 의미 없음)
1. qna
    1. 장기메모리 를 구현하다보니, 업데이트를 언제 해야 하는지? 
        1. mcp로 구현하다보니 tool calling을 할 때 저장이 되다보니, 사용자가 저장해줘 라고 하면 사용자 입장에서 불편하다보니, 사용자 정보가 있는 경우 tool calling이 자동으로 호출되게끔 구현했음 

### 연구자 특화 Deep Research MCP (Masson)

연구자를 위한 보고서 작성 에이전트, 그런데 이거 제대로 작성된 거 맞아? 

1. 이건 딥리서치가 아니에요
    1. 딥리서치와 실제 요구사항의 괴리
    2. 딥리서치 구현하는데 한계
        1. tavily , arxiv, custom search tool 등을 만들어 구현할 수 있겠으나 
            1. 누락을 피할 수 없는 아키텍처 대신 특화 워크플로우 만들어버림 
2. 클러스팅으로 충분할까?
    1. 연구노트와 메모랜덤의 구조 매핑
    2. 의미적 유사도 기반의 클러스터링을 도입 
        1. 벡터 임베딩 기반의 클러스터링 
        2. HDBSCAN - 밀도기반의 클러스터링
        3. graph 기반의 알고리즘 3가지 
    3. 
3. 제대로 작성된 거 맞아? - 평가의 어려움과 llm을 통한 평가셋 구축
    1. 클러스터링 성능의 중요성 
    2. 정성적 평가가 더 주된 요소였음
    3. llm기반의 정답데이터를 생성해서 평가르 진행했음
        1. 연구보고서 파싱 & 청킹해서 목차별 청크 추출 >임베딩 >벡터 db
        2. 최소한의 지표로 사용하기 위함 
        3. 정성평가 : 목차를 그대로 그냥 나열하는 수준 > 분류화하여 내용을 통합하지 않음 (클러스터 개수 많은 문제로 발생된 부분)
        4. graph community 방식으로 도입 > ~문서 나누는게 의미가 없을 수 있겠다
    
4. 핵심은 ‘서사’ 였어요
- 단일 클러스터 비율이 30프로 이상으로 급증 , 그래서 의미 기반의 클러스터링이 아닌 논리/서사 구조 기반의 클러스터링이 중요하다는 걸 배움!!!
- 아키텍처 변경 2번을 거침
    - 서칭 & 청킹 > 맥락 , 누락 등
    - 의미기반 클러스터링 도입 > 의미기반 목차 구성 한계
        - 최종: “논리 및 서사 구조화” 하는 방식으로 아키텍처 변경
1. QnA
    1. 벡터기반(의미기반) 클러스터링 / 구조기반 클러스터링 
        1. 서사기반의 클러스터링 
    2. 파싱을 하는 단계에서 vlm을 돌려서 캡션을 생성하는 구조로 구현 

### ESG (PangPang) - 터지지 않게 만드는 AI서비스 엔지니어링 전략

프로게이머셨음 -송준호 

- 메모리, 비용 , 멘탈 터짐
1. AI 서비스 문제
    - 무겁다: llm이 읽기전에 전처리 과정에서 out of memory 발생
    - 느리다: 사용자는 빠른 답변 원한다.
    - 예측 불가능…
    - Agent가 고도화하면서 sub agent, tool 사용, 더 커진 state 등… 메모리에 다 태우는게 힘들어짐
        - tool 실행자원은 어떻게?
        - 실행시간 길어지면 state에 context가 굉장히 길어지는데
        - tool사용하는데 비용 측정은 어떻게?
        - agent별 디버깅은 어떻게?
        - 
2. 서버리스의 배신
    1. cold start 
        1. warm start가 readiness를 의미하지 않음 (가용성이 높다는 걸 의미하는 게아님)
    2. chainlit stateful vs Ephemeral serverless(유지되어야 할 state와 사라지는 server)
    3. 자원 경합 문제/ Burst한계 
    
3. 교훈
    1. AI agent 배포에 서버리스는 적합하지 않음
    2. 서버리스는 인스턴스의 확장성만 보장 (잘 돌아가는 건 보장하지 않음)
    3. 서버리스로 올리는 건 2-3명에게 poc정도는 적합
4. VM으로 돌아와서 
    1. 문서처리 시 병목
        1. 단순 큐로 하면, 한명은 2000p넣고 다른 한 명은 1p넣으면 그 사람은 굉장히 사용성 떨어짐
            1. 어떻게 해결? 트래픽 고르게 발생하도록 큐를 2가지 유형으로 나눔
        2. 무작정 쏘면 parser rate limit 털리는 지갑
            1. redis 기반 token bucket으로 관리
        3. vm으로 구축하게 되면, 유동적인 autosclae 부재
5. 쿠버네티스
    1. 오버엔지니어링 아닌가요?
    2. chat서비스 특성 상 극단적인 트래픽 존재 / 트래픽 변동성도 매우 큼
        1. 그래서, 쿠버네티스 사용 
            1. 너무복잡 > terraform 사용
6. ECS with dify 
    1. cloudwatch - 자원누수해결
    2. managed 서비스 편리함 
7. 최적의 인프라는 있는가? 
    1. 내가 배포하고 싶은 서비스를 잘 이해하는 것이 중요 
    

### Deep Agents (Hantaek)

임한택님 - 강의 너무 잘하심 

1. Agent 개발 여정 
    1. create_react_agent > 한정된 컨텍스트 - 폭발위험 큼, 비결정성 문제 발생(제어가 불가능)
2. DeepAgents 소개
    1. 미들웨어 -  before agent/before model/after model/after agent
    2. agent harness 
    3. 핵심 3가지 : planning, middleware, sub agent 
3. 사례 살펴보기
    1. KCGS ESG 평가 에이전트
        1. 미들웨어로 규칙을 정하면, 에이전트는 그 규칙안에서 움직인다 
        
    2. PPT 생성 에이전트
        1. 사용자 요청 > 인포그래픽 PPT 자동생성 
        2. 미들웨어로 컨텍스트 해결
            1. ImageProcessingMiddlewawre
4. 결론
- 동적 주입
    - 3가지 축 : 동적 컨텍스트 주입/ 동적 툴 주입/ 동적 SubAgent 주입
- skills
    - SkillsMiddleware : 암묵지를 코드로  > deep agent에 업데이트 됨
    - 핫 키워드
- PSMS
    - Agents 업그레이드 레시피
        - Planing
        - SubAgent
        - Middleware layer
        - Skills

1. QnA
    1. react 구조는 tool calling을 되돌아 오게끔 하는 패턴 
    2. workflow랑 에이전트랑 2가지로 나뉘는데,
    3. 미들웨어가 중요한 이유? 
        1. 기존 react구조는 사이사이 로직을 넣기가 어려움 - 이 고민을 해결해 줌

### AI 프로덕트 엔지니어의 실제 역할 (Ben & Ella)

AI 모델이 아니라, AI 제품을 만든다는 것 

1. AI 프로덕트 엔지니어란?
    1. AI가 제품안에 들어왔을 때 흐름과 책임 
2. FE/BE 설계 포인트 
    1. 지연
        1. 유저의 이탈을 줄여주는
        2. llm이 동작하고 있음을 보여주는 효과를 넣어주면 좋을 것 같음 
        3. 멱등성 설계 idempotency 
            1. 캐싱전략
            2. 고유 request id 관리
        4. 비동기 처리
    2. observability
    3. HITL
3. AI 도입에 따른 업무 방식의 변화
    1. sentry webhook + issue 자동화
    2. 개발자 양극화 - 슈퍼 주니어 vs 코드 몽키 
4. 제품으로서 가치를 증명하는 법
    1. weagent 에 내장 서비스
        1. deep agent builder (자연어 기반)
        2. agent builder (노코드 기반)
5. QnA 
    1. 지연 시간 관련해서 어떻게 고민? 
        1. 느려져도 사용자가 기다리는 것 같음 (결과가 좋기만 하면)
    2. AI 프로덕트를 출시할 때, “유저전이 설계”가 중요할 거 같은데 프로덕트 디자이너가 보통 하게 될 텐데, ui와 ux가 결정되면 에이전트 아키텍처도 바뀌어야 할텐데 그러면 고민할 게 너무 많은데 어떻게 협업하고 계신지? 
        1. “협업” - FE/BE/RAG개발자 모두 같이 들어와서 고객사의 미팅을 같이 하고 서로 의견을 주고받는 핑퐁이 굉장히 많음 
            1. 핑퐁이 점점 줄어들고, 서비스 상에서 FE/BE 관점을 고려하게 되고 맞춰가게 됨 
    3. 유저의 피드백을 어떠한 방식으로 받는 지에 대한 고민? 
        1. 피드백을 해주는 회사와 협업 해서 피드백을 받았기에 피드백을 받는 부분에서는 그렇게 크게 고려하지 않았음.

### 평가 (Evaluation) (Hank)

김태한님, RAG 성능, 무엇을 어떻게 평가해야 하는가? 

- RAG에서 가장 중요하면서 어려운 주제
    - 
1. RAG 파이프라인 - 무엇을 평가해야 하는가? 
    1. RAG 성능평가 3가지
        1. 쿼리 - 청킹 - 리트리버- 프롬프트 - llm모델  파이프라인 과정에서 전략들도 굉장히 많음 
            1. 파이프라인 복잡도가 높을 수록 성능 평가하는 과정이 어려워짐
            2. 각 세부 단계에 대한 평가 중요 - 브레인크루는 3단계로 평가
                1. 문서 검색 평가 - 컨텍스트가 잘 만들어지는 지 
                2. 답변 생성 평가 - generation 평가과정 (llm이 컨텍스트를 기반으로 답변을 잘 생성하는가)
                3. end to end 평가 - 유저의 질문에 따라 답변이 잘 평가되는가 
        2. 
2. 어떻게 평가해야 하는가?
    1. 문서검색평가 
        1. precision / recall/ MRR 등 
    2. 답변 생성 평가
        1. faithfulness - 각 주장이 컨텍스트에 근거하는가?
        2. groundness - 각 주장이 컨텍스트에 의해 얼마나 지원되는가?
    3. end to end 평가
        1. relevancy 
        2. correctness
        3. semantic similarity 
    4. llm as a judge 한계 존재 (만능이 아님!!!) > 도메인 전문 특화 분야에서는 더욱 한계가 존재 함 
        1. position bias - 순서 편향
        2. verbosity bias - 답변 길이에 따른 편향
        3. self-enhancement - 자기편향(같은 모델에 답변을 좀 더 좋게 봄)
        
3. Braincrew - 적용사례
    1. langsmith tracing 
    2. 공통된 평가 체계 구축 
    3. 신뢰할 수 있는 평가체계가 가능하기에 3일만에 가능
    
4. Braincrew - next step
    1. PRep 개발 중 (평가 플랫폼)
    2. best hyper parameter combination을 찾는 게 목표 
    3. Rag 전 단계 평가 > 다양항 평가 모듈 및 벤치마크 추가 > agent 평가 확장 
    
5. QnA 
    1. llm할 때 평가가 제일 어렵고 중요한데, 평가의 목적이 사용자가 제일 만족하는 것일 것 같다. 
        1. 도메인 전문가와 테스트 데이터 셋을 구축하면서 지속적으로 고도화
    2. 좋은 테스트 데이터 셋을 구축하는 것도  도메인 전문가도 품이 많이 드는데 그런 관점에서 자동화적인 부분에서 고민하는 부분이 어떤 게 있을지 ?  
        1.
