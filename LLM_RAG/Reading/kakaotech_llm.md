
### Kakaotech 기술블로그 "LLM, 더 저렴하게, 더 빠르게, 더 똑똑하게" 읽고 정리한 글 
(원문: https://tech.kakao.com/posts/633?fbclid=IwY2xjawItVjZleHRuA2FlbQIxMQABHdoyZS5y0fgnGqU-GNAvEgNJMU6ighbUebSON3xc2J5v_vcyVOAzPH9m7g_aem_SbYDqd85W7-JiMVJBRYYyg)

* LLM은 언제든 바뀔 수 있다는 점을 염두에 두고 AI 서비스를 설계하는 것이 중요 
-> why? 
	1) LLM의 사용 가격이 지속적으로 낮아지고 있습니다. 
	2) LLM의 생성 속도가 빨라지고 있습니다.
	3) LLM의 성능이 향상되고 있습니다.
	4) LLM의 처리량이 증가하고 있습니다.
	5) 경량 LLM의 활용도가 높아지고 있습니다.
	
#### 1) LLM의 사용 가격이 지속적으로 낮아지고 있습니다.
- gpt-3.5 Turbo 초기 100만 토큰당 2달러 > gpt-4o minii 100만 토큰당 0.24달러 --10분의 1수준으로 저렴해지고 있음
-GPT-4는 첫 출시 시점에 100만 토큰당 비용은 36달러(입력과 출력 비율: 8:2 적용)의 비용 > 2024년 5월 GPT-4o가 100만 토큰당 4달러로 출시
(-> 이는 17개월 동안 약 79% 가격이 인하된 결과)
(-> 최대 24시간이 걸리는 배치 API를 사용하는 경우 다시 절반 가격인 100만 토큰당 2달러 수준으로 API를 사용할 수도 있음)
![image](https://github.com/user-attachments/assets/f6a21678-7af1-4114-8a79-4366d2d5b93c)


#### 2) LLM은 더욱 빨라지고 있습니다.
- AI를 활용한 서비스를 구축할 시, LLM의 토큰 생성 속도는 프로젝트의 중요한 핵심 요소 중 하나
- 토큰은 보통 음절 단위나 단어 수준을 의미하는데, 일반적으로 사람은 초당 30 ~ 50 토큰을 읽을 수 있으며 기존의 검색 엔진은 이보다 빠른 밀리초 단위로 수십~수백 토큰의 결과를 제공
(-> OpenAI의 GPT-4o는 초당 106 토큰을 생성하고, GPT-4o mini는 초당 131 토큰, Gemini Flash는 초당 207 토큰을 생성)
![image](https://github.com/user-attachments/assets/8fa4326e-7fa3-4397-87f5-89631396c76c)
-> 이러한 모델들의 속도 향상은 모델 자체뿐 아니라 GPU와 같은 하드웨어 성능에도 크게 좌우
-> Nvidia의 A100, H100 및 H200과 같은 고성능 GPU들이 주목을 받는 이유도 여기에 있음


#### 3) LLM의 성능이 향상되고 있습니다. 
![image](https://github.com/user-attachments/assets/26b94a67-72c5-499d-8f7e-aaa84fbaed68)

* OpenAI
-GPT-3.5(ChatGPT): 2022년 11월 말 출시, 높은 성능을 보임.
-GPT-4: 2023년 출시 이후 지속적으로 업데이트되며 벤치마크 점수 상승.
-GPT-4o (2024년 8월 6일 업데이트): 모델 성능이 크게 향상됨.
* Anthropic
-Claude 2 → Claude 3: 성능 향상과 함께 Opus, Sonet, Haiku 세 가지 모델 출시.
-Claude 3.5 Sonet: 현재 가장 뛰어난 성능 제공.
-Claude 3.5 Opus: 일부 사용자에게 한정적으로 공개되었다는 루머 존재.
* Google
-Gemini 1 (2023년 12월 출시): Ultra, Pro, Nano로 모델군을 구분.
-Gemini 1.5 Pro: 현재 가장 우수한 성능 제공.
최근 업데이트: Gemini 1.5 Pro 및 Flash 모델의 코딩 및 복잡한 프롬프트 처리 능력 향상.
* 오픈소스 LLM
-Meta: Llama 3.1
-Alibaba: Qwen2
-Mistral: Mistral Large (완전한 오픈소스는 아님)
특징: 기업이 자체 GPU 인프라를 활용하여 GPT-4급 성능을 내부적으로 활용 가능, 일부 모델은 상업적 사용도 가능.


#### 4) LLM 처리가 늘어나고 있습니다(Context 사이즈 증가)
- GPT-3.5 : (한 번에 처리할 수 있는 컨텍스트 사이즈) 4천 토큰
- GPT-4 : 8천 토큰으로 확장 > 3만 6천 토큰 > 현재(2025.02)는 12만 8천 토큰까지 처리가능
  (최근 알파 유저들에게 공개된 gpt-4o-64k-output-alpha 모델은 기존 4천 토큰 대비 6만 4천 토큰까지 확장된 아웃풋 토큰수를 사용할 수 있음)
- Anthropic의 Claude는 버전 2부터 20만 토큰 지원
- Google의 제미나이(Gemini)는 출시 당시부터 100만 토큰 지원(심지어 내부 실험에서는 1,000만 토큰까지 처리할 수 있었다고 함) > 6월부터는 Gemini 1.5 Pro 의 Context를 200만 토큰까지 지원
  
(오픈소스 LLM) 
- 최근까지 8K 토큰 지원
- 7월에 출시된 Llama 3.1이 12만 8천 토큰 지원
- Alibaba의 Qwen2의 경우, 별도의 Agent 모델을 통해 100만 토큰까지 처리 가능
- 최근 Magic 사는 Google Cloud와 협력하여, 1억 토큰에 달하는 컨텍스트 윈도우를 갖춘 코드 어시스턴트를 개발 예정
  -> 1억 토큰은 대략적으로 모델이 최대 750권의 소설에 해당하는 방대한 양의 수집된 텍스트를 이해할 수 있다는 의미 
  (*p.s: Magic의 사명 - 자동화된 소프트웨어 엔지니어 및 연구자를 개발) 
-> 이러한 대규모 컨텍스트 사이즈가 지원된다는 것은 LLM 모델이 더 많은 데이터를 한 번에 처리할 수 있다는 것을 의미
-> 지금도 제한된 컨텍스트 내에서 정확하게 정보를 처리하기 위한 다양한 RAG(Retrieval-Augmented Generation) 기술들이 개발되고 있지만, 컨텍스트 사이즈가 커지면서 일부 전문가들은 RAG의 필요성이 점차 줄어들고 있다고 의견을 공유하기도 함
-> But, 컨텍스트 사이즈가 곧 비용을 의미하기에 현재 상황에서는 **RAG 기술이 더욱 고도화될 가능성**이 높아 보임 

#### 5) 경량 LLM(sLLM)이 유용해지고 있습니다.

* 최근 AI 서비스 개발의 트렌드
- 하나의 모델만 사용하는 대신 속도, 비용, 보안 및 구현 기능에 따라 여러 LLM을 하이브리드로 결합해 사용하는 방식
  -> 대표적인 사례:  Perplexity(AI 검색 서비스 : 해당 서비스는사용자 쿼리를 빠르게 분석하는 sLLM을 앞단에 배치하여 먼저 사용자 의도를 정확히 파악 > 레퍼런스 정보 요약, 전체 내용 생성 및 후속 질문 목록 생성 등 각 기능에 맞는 다른 LLM들을 결합 > 이후, 결과를 생성하는 하이브리드 구조로 운영되는 서비스)
- 번역에 특화된 sLLM이나 빠른 응답을 위한 추론 중심의 sLLM을 활용하여 사용자 경험을 최적화할 수도 있음
- 모바일이나 노트북에 설치 가능한 On-Device 모델은 성능이 다소 떨어질 수 있지만, 빠른 응답, 개인정보 보호 및 보안을 중시하는 환경에서 특정 도메인의 성능을 강화하여 유용하게 쓰일 수 있음
  (** 대표적인 sLLM 모덷: Google의 Gemma, Gemini Nano, Microsoft의 Phi 3.5 및 Apple의 Apple Intelligence에 적용된 모델 등)

#### 결론 
- AI 서비스를 설계할 때는 빠르게 변화하는 LLM 환경에 유연하게 대응할 수 있는 아키텍처를 고려 해야 함
- LLM의 발전 속도 가속화 > AI 서비스의 가능성 확장 예정 
