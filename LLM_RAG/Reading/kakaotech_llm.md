
### Kakaotech 기술블로그 "LLM, 더 저렴하게, 더 빠르게, 더 똑똑하게" 읽고 정리한 글 
(원문: https://tech.kakao.com/posts/633?fbclid=IwY2xjawItVjZleHRuA2FlbQIxMQABHdoyZS5y0fgnGqU-GNAvEgNJMU6ighbUebSON3xc2J5v_vcyVOAzPH9m7g_aem_SbYDqd85W7-JiMVJBRYYyg)

* LLM은 언제든 바뀔 수 있다는 점을 염두에 두고 AI 서비스를 설계하는 것이 중요 
-> why? 
	1) LLM의 사용 가격이 지속적으로 낮아지고 있습니다. 
	2) LLM의 생성 속도가 빨라지고 있습니다.
	3) LLM의 성능이 향상되고 있습니다.
	4) LLM의 처리량이 증가하고 있습니다.
	5) 경량 LLM의 활용도가 높아지고 있습니다.
	
#### 1) LLM의 사용 가격이 지속적으로 낮아지고 있습니다.
- gpt-3.5 Turbo 초기 100만 토큰당 2달러 > gpt-4o minii 100만 토큰당 0.24달러 --10분의 1수준으로 저렴해지고 있음
-GPT-4는 첫 출시 시점에 100만 토큰당 비용은 36달러(입력과 출력 비율: 8:2 적용)의 비용 > 2024년 5월 GPT-4o가 100만 토큰당 4달러로 출시
(-> 이는 17개월 동안 약 79% 가격이 인하된 결과)
(-> 최대 24시간이 걸리는 배치 API를 사용하는 경우 다시 절반 가격인 100만 토큰당 2달러 수준으로 API를 사용할 수도 있음)
![image](https://github.com/user-attachments/assets/f6a21678-7af1-4114-8a79-4366d2d5b93c)



#### 2) LLM은 더욱 빨라지고 있습니다.
- AI를 활용한 서비스를 구축할 시, LLM의 토큰 생성 속도는 프로젝트의 중요한 핵심 요소 중 하나
- 토큰은 보통 음절 단위나 단어 수준을 의미하는데, 일반적으로 사람은 초당 30~50 토큰을 읽을 수 있으며 기존의 검색 엔진은 이보다 빠른 밀리초 단위로 수십~수백 토큰의 결과를 제공
(-> OpenAI의 GPT-4o는 초당 106 토큰을 생성하고, GPT-4o mini는 초당 131 토큰, Gemini Flash는 초당 207 토큰을 생성)
![image](https://github.com/user-attachments/assets/8fa4326e-7fa3-4397-87f5-89631396c76c)
-> 이러한 모델들의 속도 향상은 모델 자체뿐 아니라 GPU와 같은 하드웨어 성능에도 크게 좌우
-> Nvidia의 A100, H100 및 H200과 같은 고성능 GPU들이 주목을 받는 이유도 여기에 있음


#### 3) LLM의 성능이 향상되고 있습니다. 
![image](https://github.com/user-attachments/assets/26b94a67-72c5-499d-8f7e-aaa84fbaed68)

* OpenAI
-GPT-3.5(ChatGPT): 2022년 11월 말 출시, 높은 성능을 보임.
-GPT-4: 2023년 출시 이후 지속적으로 업데이트되며 벤치마크 점수 상승.
-GPT-4o (2024년 8월 6일 업데이트): 모델 성능이 크게 향상됨.
* Anthropic
-Claude 2 → Claude 3: 성능 향상과 함께 Opus, Sonet, Haiku 세 가지 모델 출시.
-Claude 3.5 Sonet: 현재 가장 뛰어난 성능 제공.
-Claude 3.5 Opus: 일부 사용자에게 한정적으로 공개되었다는 루머 존재.
* Google
-Gemini 1 (2023년 12월 출시): Ultra, Pro, Nano로 모델군을 구분.
-Gemini 1.5 Pro: 현재 가장 우수한 성능 제공.
최근 업데이트: Gemini 1.5 Pro 및 Flash 모델의 코딩 및 복잡한 프롬프트 처리 능력 향상.
* 오픈소스 LLM
-Meta: Llama 3.1
-Alibaba: Qwen2
-Mistral: Mistral Large (완전한 오픈소스는 아님)
특징: 기업이 자체 GPU 인프라를 활용하여 GPT-4급 성능을 내부적으로 활용 가능, 일부 모델은 상업적 사용도 가능.


(아직 정리중)
#### 4) LLM 처리가 늘어나고 있습니다(Context 사이즈 증가)

GPT-3.5가 출시될 당시, 한 번에 처리할 수 있는 컨텍스트 사이즈는 4천 토큰에 불과했습니다. 그러나 GPT-4로 넘어가면서 이 사이즈는 8천 토큰으로 확장되었고, 이후 3만 6천 토큰, 현재는 12만 8천 토큰까지 처리할 수 있게 되었습니다. 최근 알파 유저들에게 공개된 gpt-4o-64k-output-alpha 모델은 기존 4천 토큰 대비 6만 4천 토큰까지 확장된 아웃풋 토큰수를 사용할 수 있습니다.

Anthropic의 Claude는 버전 2부터 20만 토큰을 지원하고 있으며, Google의 제미나이(Gemini)는 출시 당시부터 100만 토큰이라는 파격적인 컨텍스트 사이즈를 제공했습니다. 심지어 내부 실험에서는 1,000만 토큰까지 처리할 수 있었다고 합니다. 그리고 6월부터는 Gemini 1.5 Pro 의 Context를 200만 토큰까지 지원하기 시작했습니다.

오픈소스 LLM 진영의 경우, 최근까지 8K 토큰을 지원하다가, 7월에 출시된 Llama 3.1이 12만 8천 토큰을 지원하기 시작했습니다. 또한 Alibaba의 Qwen2의 경우, 별도의 Agent 모델을 통해 100만 토큰까지 처리할 수 있습니다.

최근 Magic 사는 Google Cloud와 협력하여, 1억 토큰에 달하는 컨텍스트 윈도우를 갖춘 코드 어시스턴트를 개발하기 위해 두 대의 새로운 Google 클라우드 기반의 슈퍼컴퓨터를 구축한다고 발표했습니다. 이는 Magic의 사명인 자동화된 소프트웨어 엔지니어 및 연구자를 개발하는 데 있어 중요한 단계로 생각되는데요, 1억 토큰은 대략적으로 모델이 최대 750권의 소설에 해당하는 방대한 양의 수집된 텍스트를 이해할 수 있다는 의미입니다. 이러한 기능은 자율적인 AI 에이전트가 스스로 동작하는 데 필수적인 요소입니다.

아울러 이러한 대규모 컨텍스트 사이즈가 지원된다는 것은 LLM 모델이 더 많은 데이터를 한 번에 처리할 수 있다는 것을 의미합니다. 지금도 제한된 컨텍스트 내에서 정확하게 정보를 처리하기 위한 다양한 RAG(Retrieval-Augmented Generation) 기술들이 개발되고 있지만, 컨텍스트 사이즈가 커지면서 일부 전문가들은 RAG의 필요성이 점차 줄어들고 있다고 의견을 공유하기도 합니다. 그러나 컨텍스트 사이즈가 곧 비용을 의미하는 현재의 상황에서는 앞으로도 RAG 기술이 더욱 고도화될 가능성이 높아 보입니다.

#### 5) 경량 LLM(sLLM)이 유용해지고 있습니다.

최근 AI 서비스 개발의 트렌드를 꼽자면 하나의 모델만 사용하는 대신 속도, 비용, 보안 및 구현 기능에 따라 여러 LLM을 하이브리드로 결합해 사용하는 방식이라고 할 수 있습니다.

대표적인 사례로는 AI 검색 서비스인 Perplexity가 있습니다. 이 서비스는 사용자 쿼리를 빠르게 분석하는 sLLM을 앞단에 배치하여 먼저 사용자 의도를 정확히 파악합니다. 이후 레퍼런스 정보 요약, 전체 내용 생성 및 후속 질문 목록 생성 등 각 기능에 맞는 다른 LLM들을 결합하여 결과를 생성하는 하이브리드 구조로 운영됩니다.

또한 다른 사용 케이스들을 생각해 본다면, 번역에 특화된 sLLM이나 빠른 응답을 위한 추론 중심의 sLLM을 활용하여 사용자 경험을 최적화할 수도 있을 것입니다. 이렇게 모바일이나 노트북에 설치 가능한 On-Device 모델은 성능이 다소 떨어질 수 있지만, 빠른 응답, 개인정보 보호 및 보안을 중시하는 환경에서 특정 도메인의 성능을 강화하여 유용하게 쓰일 수 있습니다.

Google의 Gemma, Gemini Nano, Microsoft의 Phi 3.5 및 Apple의 Apple Intelligence에 적용된 모델 등이 대표적인 sLLM 모델들입니다.

지금까지 살펴본 것처럼 LLM이 빠르게 발전하면서 AI 기반 서비스의 개발에 새로운 가능성을 열고 있습니다. 가격 인하, 속도 향상, 성능 개선, 처리량 증가 및 경량 모델의 활용 증가는 AI가 앞으로 더욱더 많은 산업군에 도입될 가능성을 높이고 있습니다. 앞으로도 계속 LLM의 성능이 발전하면서 다양한 응용 분야에서 더욱 효율적이고 지능적인 솔루션을 제공하게 될 것입니다.

결론적으로, AI 서비스를 설계할 때는 빠르게 변화하는 LLM 환경에 유연하게 대응할 수 있는 아키텍처를 고려해야만 계속해서 더 저렴하고, 더 빠르고, 더 똑똑한 AI 서비스를 지속적으로 개발할 수 있을 것입니다. LLM의 발전 속도는 앞으로도 가속화될 것으로 예상되며, 이에 따라 AI 서비스의 가능성은 무한히 확장될 것입니다. 앞으로의 발전이 어떤 혁신을 가져올지 기대됩니다.
